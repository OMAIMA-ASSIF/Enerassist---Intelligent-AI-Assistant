dans main 

# Dans main.py, dans ta fonction generate()
for chunk in bot.stream({"input": request.input}, config=config):
    # Si c'est du texte normal
    if hasattr(chunk, 'content') and chunk.content:
        yield f"data: {chunk.content}\n\n"
    
    # Si c'est un appel d'outil (Création de ticket)
    if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
        yield f"data: ⚙️ Connexion au serveur MCP pour création du ticket...\n\n"


voici le main.py qui gère intelligemment les deux types de réponses (Texte vs Ticket) :

Python
import uvicorn
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from chatbot import get_chatbot_chain

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

bot = get_chatbot_chain()

class ChatRequest(BaseModel):
    input: str
    session_id: str

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    config = {"configurable": {"session_id": request.session_id}}

    def generate():
        for chunk in bot.stream({"input": request.input}, config=config):
            # 1. Si l'IA envoie du texte (Diagnostic)
            if hasattr(chunk, 'content') and chunk.content:
                yield f"data: {chunk.content}\n\n"
            
            # 2. Si l'IA déclenche l'outil (Ticketing)
            if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
                yield f"data: ⚙️ Génération du ticket Jira en cours...\n\n"
                # Note: Dans un flux streaming standard, l'exécution de l'outil 
                # se fait après ou via un agent spécialisé. 
                # Ici, cela informe l'utilisateur de l'action.

    return StreamingResponse(generate(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)